"""
query.py â€“ CLI tool to ask questions against the ChromaDB knowledge base.

Usage:
    python query.py "CÃ¢u há»i cá»§a báº¡n?"
    python query.py "CÃ¢u há»i?" --model mistral
    python query.py "CÃ¢u há»i?" --model qwen2.5:3b --top-k 3 --show-chunks
"""

import argparse
from rag_pipeline import ask_ollama, retrieve, QWEN_MODEL, MISTRAL_MODEL


def main():
    parser = argparse.ArgumentParser(
        description="Äáº·t cÃ¢u há»i vÃ  nháº­n tráº£ lá»i tá»« ChromaDB + LLM (Qwen / Mistral)"
    )
    parser.add_argument("question", help="CÃ¢u há»i cáº§n tráº£ lá»i")
    parser.add_argument(
        "--model", "-m", default=QWEN_MODEL,
        choices=[QWEN_MODEL, MISTRAL_MODEL, "qwen2.5:3b", "mistral"],
        help=f"LLM model Ollama sá»­ dá»¥ng (máº·c Ä‘á»‹nh: {QWEN_MODEL})",
    )
    parser.add_argument(
        "--top-k", "-k", type=int, default=5,
        help="Sá»‘ lÆ°á»£ng chunks liÃªn quan nháº¥t truy váº¥n tá»« ChromaDB (máº·c Ä‘á»‹nh: 5)",
    )
    parser.add_argument(
        "--show-chunks", action="store_true",
        help="Hiá»ƒn thá»‹ cÃ¡c chunks liÃªn quan tÃ¬m Ä‘Æ°á»£c tá»« ChromaDB",
    )
    parser.add_argument(
        "--retrieve-only", action="store_true",
        help="Chá»‰ tÃ¬m chunks liÃªn quan, khÃ´ng gá»i LLM",
    )

    args = parser.parse_args()

    print(f"\nğŸ” CÃ¢u há»i: {args.question}")
    print(f"   Model  : {args.model}  |  Top-K: {args.top_k}\n")

    if args.retrieve_only:
        hits = retrieve(args.question, top_k=args.top_k)
        print(f"ğŸ“„ {len(hits)} Ä‘oáº¡n vÄƒn liÃªn quan nháº¥t tá»« ChromaDB:\n")
        for i, h in enumerate(hits, 1):
            src = h["metadata"].get("source", "?")
            idx = h["metadata"].get("chunk_index", "?")
            dist = h["distance"]
            print(f"â”€â”€â”€ Chunk {i}  [nguá»“n: {src}  idx: {idx}  khoáº£ng cÃ¡ch: {dist:.4f}] â”€â”€â”€")
            print(h["text"])
            print()
        return

    result = ask_ollama(args.question, model=args.model, top_k=args.top_k)

    if args.show_chunks:
        print(f"ğŸ“„ {len(result['chunks_used'])} Ä‘oáº¡n vÄƒn liÃªn quan nháº¥t tá»« ChromaDB:\n")
        for i, h in enumerate(result["chunks_used"], 1):
            src = h["metadata"].get("source", "?")
            idx = h["metadata"].get("chunk_index", "?")
            dist = h["distance"]
            print(f"â”€â”€â”€ Chunk {i}  [nguá»“n: {src}  idx: {idx}  khoáº£ng cÃ¡ch: {dist:.4f}] â”€â”€â”€")
            print(h["text"])
            print()

    print(f"ğŸ¤– Tráº£ lá»i ({result['model']}):\n")
    print(result["answer"])
    print()


if __name__ == "__main__":
    main()
